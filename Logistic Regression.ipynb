{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3215934b",
   "metadata": {},
   "source": [
    "# Student Risk Prediction using Logistic Regression and Random Forest Classifier\n",
    "\n",
    "**UCI Student Performance Dataset (student-mat.csv)**\n",
    "\n",
    "This notebook implements binary classification models to predict student risk based on demographic, social, and academic features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7281f4",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e7520b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, LabelEncoder\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    confusion_matrix, \n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039cfe5c",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2af607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '../Student Performance Prediction/Data/student-mat.csv'\n",
    "df = pd.read_csv(data_path, sep=';')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617db53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nBasic Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257846bf",
   "metadata": {},
   "source": [
    "## 3. Create Target Variable for Risk Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4a46a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary target variable\n",
    "# Students with final grade (G3) below 12 are classified as 'at risk'\n",
    "threshold = 12\n",
    "y = (df['G3'] < threshold).astype(int)\n",
    "\n",
    "print(f\"Creating target variable (threshold = {threshold}):\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nRisk prevalence: {y.mean()*100:.2f}% of students are at risk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60767b9",
   "metadata": {},
   "source": [
    "## 4. Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e9fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features (exclude target and intermediate grades)\n",
    "exclude_cols = ['G1', 'G2', 'G3']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "\n",
    "print(f\"Number of features: {len(X.columns)}\")\n",
    "print(f\"\\nFeatures selected:\")\n",
    "print(list(X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1e407",
   "metadata": {},
   "source": [
    "## 5. Preprocess Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2959870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numeric columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Categorical columns: {categorical_cols}\\n\")\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"Encoded '{col}': {len(le.classes_)} classes\")\n",
    "\n",
    "print(\"\\nFeatures preprocessed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471476c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Handling missing values...\")\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().sum() > 0:\n",
    "        if X[col].dtype in ['int64', 'float64']:\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "        else:\n",
    "            X[col].fillna(X[col].mode()[0], inplace=True)\n",
    "\n",
    "print(f\"\\nMissing values after handling:\")\n",
    "print(X.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa0eb96",
   "metadata": {},
   "source": [
    "## 6. Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac766b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with stratification to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTesting set class distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa1395c",
   "metadata": {},
   "source": [
    "## 7. Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b950e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler to normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled successfully!\")\n",
    "print(f\"\\nScaled training set statistics:\")\n",
    "print(f\"Mean: {X_train_scaled.mean(axis=0).mean():.6f}\")\n",
    "print(f\"Std: {X_train_scaled.std(axis=0).mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac3ec9",
   "metadata": {},
   "source": [
    "## 8. Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47acb8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Logistic Regression model trained successfully!\")\n",
    "print(f\"Number of iterations: {lr_model.n_iter_[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d021ca",
   "metadata": {},
   "source": [
    "## 9. Train Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e15ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Random Forest model trained successfully!\")\n",
    "print(f\"Number of trees: {rf_model.n_estimators}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc3c7aa",
   "metadata": {},
   "source": [
    "## 10. Evaluate Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa12327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "lr_precision = precision_score(y_test, y_pred_lr)\n",
    "lr_recall = recall_score(y_test, y_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "lr_roc_auc = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOGISTIC REGRESSION - EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAccuracy:  {lr_accuracy:.4f}\")\n",
    "print(f\"Precision: {lr_precision:.4f}\")\n",
    "print(f\"Recall:    {lr_recall:.4f}\")\n",
    "print(f\"F1-Score:  {lr_f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {lr_roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Classification Report\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm_lr)\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3467e4",
   "metadata": {},
   "source": [
    "## 11. Evaluate Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15290e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "rf_precision = precision_score(y_test, y_pred_rf)\n",
    "rf_recall = recall_score(y_test, y_pred_rf)\n",
    "rf_f1 = f1_score(y_test, y_pred_rf)\n",
    "rf_roc_auc = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RANDOM FOREST - EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAccuracy:  {rf_accuracy:.4f}\")\n",
    "print(f\"Precision: {rf_precision:.4f}\")\n",
    "print(f\"Recall:    {rf_recall:.4f}\")\n",
    "print(f\"F1-Score:  {rf_f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {rf_roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c952cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Classification Report\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm_rf)\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b23999",
   "metadata": {},
   "source": [
    "## 12. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a922e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Logistic Regression': {\n",
    "        'Accuracy': lr_accuracy,\n",
    "        'Precision': lr_precision,\n",
    "        'Recall': lr_recall,\n",
    "        'F1-Score': lr_f1,\n",
    "        'ROC-AUC': lr_roc_auc\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'Accuracy': rf_accuracy,\n",
    "        'Precision': rf_precision,\n",
    "        'Recall': rf_recall,\n",
    "        'F1-Score': rf_f1,\n",
    "        'ROC-AUC': rf_roc_auc\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ae09b6",
   "metadata": {},
   "source": [
    "## 13. Visualizations - Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ca2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Accuracy comparison\n",
    "ax = axes[0, 0]\n",
    "models = ['Logistic Regression', 'Random Forest']\n",
    "accuracies = [lr_accuracy, rf_accuracy]\n",
    "ax.bar(models, accuracies, color=['#3498db', '#e74c3c'])\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy Comparison')\n",
    "ax.set_ylim([0, 1])\n",
    "for i, v in enumerate(accuracies):\n",
    "    ax.text(i, v + 0.02, f'{v:.4f}', ha='center')\n",
    "\n",
    "# Precision-Recall comparison\n",
    "ax = axes[0, 1]\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "precisions = [lr_precision, rf_precision]\n",
    "recalls = [lr_recall, rf_recall]\n",
    "ax.bar(x - width/2, precisions, width, label='Precision', color='#2ecc71')\n",
    "ax.bar(x + width/2, recalls, width, label='Recall', color='#f39c12')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Precision vs Recall')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# F1-Score comparison\n",
    "ax = axes[1, 0]\n",
    "f1_scores = [lr_f1, rf_f1]\n",
    "ax.bar(models, f1_scores, color=['#3498db', '#e74c3c'])\n",
    "ax.set_ylabel('F1-Score')\n",
    "ax.set_title('F1-Score Comparison')\n",
    "ax.set_ylim([0, 1])\n",
    "for i, v in enumerate(f1_scores):\n",
    "    ax.text(i, v + 0.02, f'{v:.4f}', ha='center')\n",
    "\n",
    "# ROC-AUC comparison\n",
    "ax = axes[1, 1]\n",
    "roc_aucs = [lr_roc_auc, rf_roc_auc]\n",
    "ax.bar(models, roc_aucs, color=['#3498db', '#e74c3c'])\n",
    "ax.set_ylabel('ROC-AUC')\n",
    "ax.set_title('ROC-AUC Comparison')\n",
    "ax.set_ylim([0, 1])\n",
    "for i, v in enumerate(roc_aucs):\n",
    "    ax.text(i, v + 0.02, f'{v:.4f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dd27f0",
   "metadata": {},
   "source": [
    "## 14. Visualizations - Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d5a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle('Confusion Matrices', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Logistic Regression confusion matrix\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0], cbar=False)\n",
    "axes[0].set_title('Logistic Regression')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Random Forest confusion matrix\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Reds', ax=axes[1], cbar=False)\n",
    "axes[1].set_title('Random Forest Classifier')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a7a15",
   "metadata": {},
   "source": [
    "## 15. Visualizations - ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74045995",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Logistic Regression ROC curve\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)\n",
    "plt.plot(fpr_lr, tpr_lr, label=f\"Logistic Regression (AUC = {lr_roc_auc:.4f})\", \n",
    "        linewidth=2, color='#3498db')\n",
    "\n",
    "# Random Forest ROC curve\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest (AUC = {rf_roc_auc:.4f})\", \n",
    "        linewidth=2, color='#e74c3c')\n",
    "\n",
    "# Diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Student Risk Prediction', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46e7808",
   "metadata": {},
   "source": [
    "## 16. Feature Importance Analysis (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TOP 15 IMPORTANT FEATURES (Random Forest)\")\n",
    "print(\"=\"*60)\n",
    "print(feature_importance.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44b0105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'].values, color='#9b59b6')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.title('Top 15 Feature Importance - Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17509a0d",
   "metadata": {},
   "source": [
    "## 17. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a82327",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ANALYSIS SUMMARY - STUDENT RISK PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. DATASET OVERVIEW:\")\n",
    "print(f\"   - Total students: {len(df)}\")\n",
    "print(f\"   - At-risk students: {y.sum()} ({y.mean()*100:.2f}%)\")\n",
    "print(f\"   - Total features used: {len(X.columns)}\")\n",
    "\n",
    "print(\"\\n2. DATA SPLIT:\")\n",
    "print(f\"   - Training samples: {len(X_train)} (80%)\")\n",
    "print(f\"   - Testing samples: {len(X_test)} (20%)\")\n",
    "\n",
    "print(\"\\n3. MODEL PERFORMANCE - LOGISTIC REGRESSION:\")\n",
    "print(f\"   - Accuracy:  {lr_accuracy:.4f}\")\n",
    "print(f\"   - Precision: {lr_precision:.4f}\")\n",
    "print(f\"   - Recall:    {lr_recall:.4f}\")\n",
    "print(f\"   - F1-Score:  {lr_f1:.4f}\")\n",
    "print(f\"   - ROC-AUC:   {lr_roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\n4. MODEL PERFORMANCE - RANDOM FOREST:\")\n",
    "print(f\"   - Accuracy:  {rf_accuracy:.4f}\")\n",
    "print(f\"   - Precision: {rf_precision:.4f}\")\n",
    "print(f\"   - Recall:    {rf_recall:.4f}\")\n",
    "print(f\"   - F1-Score:  {rf_f1:.4f}\")\n",
    "print(f\"   - ROC-AUC:   {rf_roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\n5. BEST MODEL:\")\n",
    "if rf_roc_auc > lr_roc_auc:\n",
    "    print(f\"   Random Forest outperforms with ROC-AUC: {rf_roc_auc:.4f}\")\n",
    "else:\n",
    "    print(f\"   Logistic Regression outperforms with ROC-AUC: {lr_roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
